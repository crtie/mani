OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:31 - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0,1: Tesla V100S-PCIE-32GB
CUDA_HOME: /usr/local/cuda-11.2
NVCC: Build cuda_11.2.r11.2/compiler.29618528_0
Num of GPUs: 2
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.3-Product Build 20210617 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu102
OpenCV: 4.5.3
mani_skill_learn: 1.0.0
------------------------------------------------------------

OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:31 - Config:
agent = dict(
    type='MBPO',
    batch_size=256,
    gamma=0.95,
    update_coeff=0.005,
    alpha=0.2,
    target_update_interval=1,
    automatic_alpha_tuning=True,
    alpha_optim_cfg=dict(type='Adam', lr=0.0003),
    max_iter_use_real_data=1000,
    policy_cfg=dict(
        type='ContinuousPolicy',
        policy_head_cfg=dict(
            type='GaussianHead', log_sig_min=-20, log_sig_max=2,
            epsilon=1e-06),
        nn_cfg=dict(
            type='PointNetWithInstanceInfoV0',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=['agent_shape + pcd_xyz_rgb_channel', 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[192, 128, 'action_shape * 2'],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0003, weight_decay=5e-06)),
    value_cfg=dict(
        type='ContinuousValue',
        num_heads=2,
        nn_cfg=dict(
            type='PointNetWithInstanceInfoV0',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=[
                        'agent_shape + pcd_xyz_rgb_channel + action_shape',
                        192, 192
                    ],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape + action_shape', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[192, 128, 1],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0005, weight_decay=5e-06)),
    model_cfg=dict(
        type='Pointnet_transformer_model',
        num_heads=1,
        nn_cfg=dict(
            type='PointNetWorldModel',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=[
                        'agent_shape + pcd_xyz_rgb_channel + action_shape',
                        192, 192
                    ],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape + action_shape ', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp1_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[
                    'agent_shape + pcd_all_channel + action_shape + 192', 128,
                    3
                ],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            final_mlp2_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[
                    'agent_shape + action_shape + 192', 128, 'agent_shape +1 '
                ],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0005, weight_decay=5e-06)))
log_level = 'INFO'
train_mfrl_cfg = dict(
    on_policy=False,
    total_steps=2000000,
    warm_steps=4000,
    n_eval=200000,
    n_checkpoint=20000,
    n_steps=8,
    n_updates=4,
    m_steps=1)
rollout_cfg = dict(
    type='BatchRollout',
    use_cost=False,
    reward_only=False,
    num_procs=8,
    with_info=False,
    env_cfg=dict(
        type='gym',
        unwrapped=False,
        obs_mode='pointcloud',
        reward_type='dense',
        stack_frame=1,
        env_name='OpenCabinetDrawer-v0'))
eval_cfg = dict(
    type='BatchEvaluation',
    num=100,
    num_procs=8,
    use_hidden_state=False,
    start_state=None,
    save_traj=False,
    save_video=False,
    use_log=True,
    env_cfg=dict(
        type='gym',
        unwrapped=False,
        obs_mode='pointcloud',
        reward_type='dense',
        stack_frame=1,
        env_name='OpenCabinetDrawer-v0'))
stack_frame = 1
num_heads = 4
env_cfg = dict(
    type='gym',
    unwrapped=False,
    obs_mode='pointcloud',
    reward_type='dense',
    stack_frame=1,
    env_name='OpenCabinetDrawer-v0')
replay_cfg = dict(type='ReplayMemory', capacity=500000)
replay_model_cfg = dict(type='ReplayMemory', capacity=500000)
expert_replay_split_cfg = dict(type='ReplayMemory', capacity=30000)
resume_from = 'MBPO.ckpt'
work_dir = './work_dirs/OpenCabinetDrawer-v0/MBPO'

OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:31 - Set random seed to 0
OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:42 - State shape:{'pointcloud': {'rgb': (1200, 3), 'xyz': (1200, 3), 'seg': (1200, 3)}, 'state': 38}, action shape:Box(-1.0, 1.0, (13,), float32)
OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:46 - We do not use distributed training, but we support data parallel in torch
OpenCabinetDrawer-v0 - WARNING - 2022-05-05 09:52:46 - Use Data parallel to train model! It may slow down the speed when the model is small.
OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:46 - MBPO(
  (policy): CustomDataParallel(
    (module): ContinuousPolicy(
      (backbone): PointNetWithInstanceInfoV0(
        (pcd_pns): ModuleList(
          (0): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (1): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (2): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (3): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (4): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
        )
        (attn): TransformerEncoder(
          (attn_blocks): ModuleList(
            (0): TransformerBlock(
              (attn): MultiHeadSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=768, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=768, out_features=192, bias=True)
                )
              )
              (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): TransformerBlock(
              (attn): MultiHeadSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=768, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=768, out_features=192, bias=True)
                )
              )
              (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (pooling): AttentionPooling(
            (dropout): Identity()
          )
        )
        (state_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=38, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=128, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=128, out_features=26, bias=True)
          )
        )
      )
      (policy_head): GaussianHead()
    )
  )
  (critic): CustomDataParallel(
    (module): ContinuousValue(
      (values): ModuleList(
        (0): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
        (1): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
      )
    )
  )
  (model): CustomDataParallel(
    (module): Pointnet_transformer_model(
      (values): ModuleList(
        (0): PointNetWorldModel(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp1): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=252, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=3, bias=True)
            )
          )
          (global_mlp2): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=243, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=39, bias=True)
            )
          )
        )
      )
    )
  )
  (target_critic): CustomDataParallel(
    (module): ContinuousValue(
      (values): ModuleList(
        (0): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
        (1): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
      )
    )
  )
)
OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:52:46 - Rollout state dim: {'pointcloud': {'rgb': (8, 1200, 3), 'xyz': (8, 1200, 3), 'seg': (8, 1200, 3)}, 'state': (8, 38)}, action dim: 8
OpenCabinetDrawer-v0 - INFO - 2022-05-05 09:54:07 - Finish 4000 warm-up steps!
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:00:37 - 5600/2000000(0%) Passed time:6m30s ETA:5d15h24m49s pred loss1:0.06 pred loss2:0.04 pred loss3:5.5920e-04 episode_length:154.00 episode_reward:-1555.08 critic_loss:11.95 max_critic_abs_err:16.66 policy_loss:176.31 alpha:0.02 alpha_loss:-0.03 q:-177.36 q_target:-176.60 log_pi:14.58 memory:39.86G gpu_mem_ratio:14.3% gpu_mem:4.53G gpu_mem_this:4.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:07:10 - 7200/2000000(0%) Passed time:13m2s ETA:5d15h38m57s pred loss1:0.14 pred loss2:0.01 pred loss3:5.7640e-04 episode_length:184.00 episode_reward:-1830.01 critic_loss:7.86 max_critic_abs_err:15.90 policy_loss:177.86 alpha:0.02 alpha_loss:0.02 q:-178.60 q_target:-177.98 log_pi:11.75 memory:40.64G gpu_mem_ratio:15.1% gpu_mem:4.78G gpu_mem_this:4.77G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:13:42 - 8800/2000000(0%) Passed time:19m34s ETA:5d15h39m51s pred loss1:0.06 pred loss2:0.02 pred loss3:5.5986e-04 episode_length:181.67 episode_reward:-1798.16 critic_loss:10.04 max_critic_abs_err:19.52 policy_loss:178.55 alpha:0.02 alpha_loss:5.6235e-03 q:-179.58 q_target:-178.80 log_pi:12.69 memory:41.68G gpu_mem_ratio:17.4% gpu_mem:5.53G gpu_mem_this:5.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:20:15 - 10400/2000000(1%) Passed time:26m7s ETA:5d15h39m55s pred loss1:0.09 pred loss2:0.02 pred loss3:5.7317e-04 episode_length:159.88 episode_reward:-1559.71 critic_loss:7.41 max_critic_abs_err:15.09 policy_loss:179.48 alpha:0.02 alpha_loss:0.03 q:-180.17 q_target:-179.44 log_pi:11.26 memory:42.16G gpu_mem_ratio:17.4% gpu_mem:5.53G gpu_mem_this:5.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:25:09 - 11568/2000000(1%) Passed time:31m2s ETA:5d16h12m25s pred loss1:0.17 pred loss2:0.02 pred loss3:5.6605e-04 episode_length:178.75 episode_reward:-1754.28 critic_loss:12.42 max_critic_abs_err:19.81 policy_loss:179.27 alpha:0.02 alpha_loss:-6.6039e-04 q:-179.91 q_target:-179.10 log_pi:13.04 memory:42.99G gpu_mem_ratio:18.2% gpu_mem:5.78G gpu_mem_this:5.77G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:31:42 - 13168/2000000(1%) Passed time:37m35s ETA:5d16h2m20s pred loss1:0.14 pred loss2:0.02 pred loss3:5.8553e-04 episode_length:149.88 episode_reward:-1552.04 critic_loss:37.12 max_critic_abs_err:73.29 policy_loss:181.65 alpha:0.02 alpha_loss:3.2044e-03 q:-182.66 q_target:-181.58 log_pi:12.81 memory:44.16G gpu_mem_ratio:19.8% gpu_mem:6.28G gpu_mem_this:6.27G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:37:41 - 14632/2000000(1%) Passed time:43m33s ETA:5d15h51m44s pred loss1:0.02 pred loss2:0.02 pred loss3:5.8533e-04 episode_length:181.50 episode_reward:-1892.67 critic_loss:12.51 max_critic_abs_err:26.44 policy_loss:183.03 alpha:0.02 alpha_loss:-4.5558e-03 q:-183.59 q_target:-182.74 log_pi:13.27 memory:44.33G gpu_mem_ratio:19.8% gpu_mem:6.28G gpu_mem_this:6.27G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:44:13 - 16232/2000000(1%) Passed time:50m6s ETA:5d15h42m4s pred loss1:0.06 pred loss2:0.06 pred loss3:6.0489e-04 episode_length:199.50 episode_reward:-2081.60 critic_loss:11.92 max_critic_abs_err:17.91 policy_loss:185.46 alpha:0.02 alpha_loss:-0.02 q:-186.21 q_target:-185.45 log_pi:14.56 memory:44.93G gpu_mem_ratio:19.8% gpu_mem:6.28G gpu_mem_this:6.27G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:50:38 - 17808/2000000(1%) Passed time:56m31s ETA:5d15h30m26s pred loss1:0.06 pred loss2:0.02 pred loss3:6.4779e-04 episode_length:199.62 episode_reward:-2208.00 critic_loss:15.11 max_critic_abs_err:20.99 policy_loss:188.13 alpha:0.02 alpha_loss:-0.01 q:-189.18 q_target:-188.19 log_pi:13.57 memory:45.40G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 10:55:29 - 18976/2000000(1%) Passed time:1h1m22s ETA:5d15h34m57s pred loss1:0.03 pred loss2:0.03 pred loss3:5.9146e-04 episode_length:174.38 episode_reward:-1741.35 critic_loss:30.20 max_critic_abs_err:35.60 policy_loss:188.58 alpha:0.02 alpha_loss:-0.02 q:-188.70 q_target:-187.65 log_pi:14.28 memory:45.86G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:02:02 - 20576/2000000(1%) Passed time:1h7m54s ETA:5d15h26m8s pred loss1:0.02 pred loss2:0.02 pred loss3:6.0049e-04 episode_length:200.00 episode_reward:-2116.47 critic_loss:7.78 max_critic_abs_err:14.90 policy_loss:188.80 alpha:0.02 alpha_loss:-8.0596e-03 q:-189.83 q_target:-189.09 log_pi:13.48 memory:46.53G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:02:02 - Save model at step: 20576.The model will be saved at ./work_dirs/OpenCabinetDrawer-v0/MBPO/models/model_20000.ckpt
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:07:33 - 21912/2000000(1%) Passed time:1h13m26s ETA:5d15h26m47s pred loss1:0.03 pred loss2:0.02 pred loss3:6.6497e-04 episode_length:182.00 episode_reward:-1806.26 critic_loss:10.37 max_critic_abs_err:16.81 policy_loss:187.43 alpha:0.02 alpha_loss:-0.02 q:-188.55 q_target:-187.88 log_pi:14.12 memory:46.83G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:11:34 - 22872/2000000(1%) Passed time:1h17m27s ETA:5d15h30m47s pred loss1:0.02 pred loss2:0.03 pred loss3:6.1320e-04 episode_length:130.62 episode_reward:-1279.57 critic_loss:14.01 max_critic_abs_err:16.86 policy_loss:186.37 alpha:0.02 alpha_loss:-0.02 q:-187.57 q_target:-186.45 log_pi:13.89 memory:46.95G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:17:45 - 24384/2000000(1%) Passed time:1h23m38s ETA:5d15h22m11s pred loss1:0.12 pred loss2:0.02 pred loss3:6.0559e-04 episode_length:158.50 episode_reward:-1518.06 critic_loss:22.35 max_critic_abs_err:31.47 policy_loss:184.23 alpha:0.02 alpha_loss:-0.01 q:-185.49 q_target:-184.50 log_pi:13.55 memory:47.22G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:20:59 - 25120/2000000(1%) Passed time:1h26m51s ETA:5d15h38m46s pred loss1:0.07 pred loss2:0.03 pred loss3:6.1396e-04 episode_length:113.00 episode_reward:-1181.98 critic_loss:19.68 max_critic_abs_err:28.26 policy_loss:183.83 alpha:0.02 alpha_loss:-0.04 q:-185.06 q_target:-184.20 log_pi:14.93 memory:47.44G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:27:30 - 26720/2000000(1%) Passed time:1h33m22s ETA:5d15h26m35s pred loss1:0.02 pred loss2:0.02 pred loss3:6.2663e-04 episode_length:181.50 episode_reward:-1881.30 critic_loss:14.65 max_critic_abs_err:22.48 policy_loss:187.14 alpha:0.02 alpha_loss:0.03 q:-188.29 q_target:-187.24 log_pi:11.70 memory:47.60G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:32:46 - 27992/2000000(1%) Passed time:1h38m38s ETA:5d15h24m39s pred loss1:0.04 pred loss2:0.03 pred loss3:5.9446e-04 episode_length:156.88 episode_reward:-1617.61 critic_loss:26.54 max_critic_abs_err:30.45 policy_loss:183.82 alpha:0.02 alpha_loss:-0.02 q:-185.43 q_target:-184.35 log_pi:14.09 memory:47.78G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:37:34 - 29144/2000000(1%) Passed time:1h43m27s ETA:5d15h25m27s pred loss1:0.05 pred loss2:0.02 pred loss3:6.0309e-04 episode_length:120.88 episode_reward:-1193.92 critic_loss:18.11 max_critic_abs_err:23.54 policy_loss:183.58 alpha:0.02 alpha_loss:-0.01 q:-184.98 q_target:-183.94 log_pi:13.65 memory:48.07G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:42:10 - 30240/2000000(2%) Passed time:1h48m2s ETA:5d15h27m3s pred loss1:0.04 pred loss2:0.02 pred loss3:6.2456e-04 episode_length:164.38 episode_reward:-1618.89 critic_loss:11.87 max_critic_abs_err:15.53 policy_loss:184.14 alpha:0.02 alpha_loss:4.7541e-03 q:-185.63 q_target:-184.84 log_pi:12.79 memory:48.24G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:46:12 - 31192/2000000(2%) Passed time:1h52m4s ETA:5d15h31m43s pred loss1:0.03 pred loss2:0.03 pred loss3:6.5503e-04 episode_length:156.75 episode_reward:-1526.50 critic_loss:18.21 max_critic_abs_err:25.58 policy_loss:182.59 alpha:0.02 alpha_loss:-0.01 q:-184.12 q_target:-183.29 log_pi:13.51 memory:48.42G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:51:28 - 32472/2000000(2%) Passed time:1h57m21s ETA:5d15h25m58s pred loss1:0.05 pred loss2:0.03 pred loss3:6.1814e-04 episode_length:151.00 episode_reward:-1474.24 critic_loss:11.20 max_critic_abs_err:15.37 policy_loss:183.08 alpha:0.02 alpha_loss:6.4817e-03 q:-184.60 q_target:-183.79 log_pi:12.70 memory:48.51G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 11:57:40 - 33984/2000000(2%) Passed time:2h3m32s ETA:5d15h17m9s pred loss1:0.05 pred loss2:0.02 pred loss3:5.8973e-04 episode_length:181.25 episode_reward:-1813.72 critic_loss:13.44 max_critic_abs_err:25.56 policy_loss:181.20 alpha:0.02 alpha_loss:2.2664e-04 q:-182.78 q_target:-181.90 log_pi:12.99 memory:49.12G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:02:06 - 35040/2000000(2%) Passed time:2h7m58s ETA:5d15h18m5s pred loss1:0.14 pred loss2:0.02 pred loss3:5.6499e-04 episode_length:123.50 episode_reward:-1191.99 critic_loss:9.43 max_critic_abs_err:13.88 policy_loss:181.39 alpha:0.02 alpha_loss:9.9828e-03 q:-183.02 q_target:-182.08 log_pi:12.55 memory:49.23G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:06:49 - 36176/2000000(2%) Passed time:2h12m41s ETA:5d15h15m39s pred loss1:0.16 pred loss2:0.04 pred loss3:6.2671e-04 episode_length:144.38 episode_reward:-1365.63 critic_loss:10.22 max_critic_abs_err:15.61 policy_loss:181.06 alpha:0.02 alpha_loss:6.4375e-03 q:-182.12 q_target:-181.32 log_pi:12.70 memory:49.47G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:12:00 - 37432/2000000(2%) Passed time:2h17m53s ETA:5d15h10m48s pred loss1:0.03 pred loss2:0.02 pred loss3:5.7914e-04 episode_length:179.25 episode_reward:-1827.35 critic_loss:18.98 max_critic_abs_err:21.30 policy_loss:180.31 alpha:0.02 alpha_loss:1.4046e-03 q:-181.59 q_target:-180.47 log_pi:12.93 memory:49.60G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:18:30 - 39032/2000000(2%) Passed time:2h24m23s ETA:5d14h58m53s pred loss1:0.08 pred loss2:0.02 pred loss3:6.2032e-04 episode_length:158.25 episode_reward:-1630.50 critic_loss:18.07 max_critic_abs_err:32.32 policy_loss:188.98 alpha:0.02 alpha_loss:0.04 q:-189.62 q_target:-188.73 log_pi:11.06 memory:49.81G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:24:21 - 40456/2000000(2%) Passed time:2h30m13s ETA:5d14h51m27s pred loss1:0.07 pred loss2:0.05 pred loss3:5.9643e-04 episode_length:174.88 episode_reward:-1788.98 critic_loss:19.39 max_critic_abs_err:27.95 policy_loss:187.32 alpha:0.02 alpha_loss:0.01 q:-188.32 q_target:-187.24 log_pi:12.33 memory:50.11G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:24:21 - Save model at step: 40456.The model will be saved at ./work_dirs/OpenCabinetDrawer-v0/MBPO/models/model_40000.ckpt
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:30:52 - 42056/2000000(2%) Passed time:2h36m45s ETA:5d14h41m21s pred loss1:0.08 pred loss2:0.03 pred loss3:6.0971e-04 episode_length:200.00 episode_reward:-2058.38 critic_loss:16.53 max_critic_abs_err:21.73 policy_loss:186.66 alpha:0.02 alpha_loss:9.0400e-03 q:-187.82 q_target:-186.91 log_pi:12.47 memory:50.26G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:33:07 - 42544/2000000(2%) Passed time:2h39m ETA:5d14h51m25s pred loss1:0.03 pred loss2:0.03 pred loss3:6.0037e-04 episode_length:122.62 episode_reward:-1260.52 critic_loss:15.06 max_critic_abs_err:19.54 policy_loss:188.19 alpha:0.02 alpha_loss:-5.4838e-03 q:-189.25 q_target:-188.25 log_pi:13.32 memory:50.43G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:39:33 - 44128/2000000(2%) Passed time:2h45m26s ETA:5d14h40m6s pred loss1:0.04 pred loss2:0.03 pred loss3:6.0437e-04 episode_length:180.75 episode_reward:-1872.98 critic_loss:19.34 max_critic_abs_err:21.41 policy_loss:189.71 alpha:0.02 alpha_loss:-0.01 q:-190.53 q_target:-189.56 log_pi:13.67 memory:50.55G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:45:05 - 45472/2000000(2%) Passed time:2h50m57s ETA:5d14h33m41s pred loss1:0.03 pred loss2:0.02 pred loss3:5.9698e-04 episode_length:127.00 episode_reward:-1293.13 critic_loss:20.81 max_critic_abs_err:25.57 policy_loss:189.47 alpha:0.02 alpha_loss:4.9522e-03 q:-190.27 q_target:-189.31 log_pi:12.71 memory:50.76G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:48:32 - 46288/2000000(2%) Passed time:2h54m24s ETA:5d14h34m14s pred loss1:0.02 pred loss2:0.02 pred loss3:5.7970e-04 episode_length:132.25 episode_reward:-1370.12 critic_loss:45.44 max_critic_abs_err:53.31 policy_loss:190.35 alpha:0.02 alpha_loss:8.1677e-03 q:-191.32 q_target:-190.03 log_pi:12.50 memory:50.84G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 12:54:59 - 47888/2000000(2%) Passed time:3h51s ETA:5d14h21m12s pred loss1:0.02 pred loss2:0.02 pred loss3:5.7136e-04 episode_length:194.50 episode_reward:-2044.31 critic_loss:57.47 max_critic_abs_err:61.72 policy_loss:190.48 alpha:0.02 alpha_loss:-0.01 q:-190.77 q_target:-189.65 log_pi:13.81 memory:50.96G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:01:26 - 49488/2000000(2%) Passed time:3h7m19s ETA:5d14h8m53s pred loss1:0.03 pred loss2:0.03 pred loss3:6.2118e-04 episode_length:167.38 episode_reward:-1805.16 critic_loss:23.94 max_critic_abs_err:30.90 policy_loss:192.07 alpha:0.02 alpha_loss:2.6292e-03 q:-193.11 q_target:-192.06 log_pi:12.83 memory:51.18G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:06:15 - 50640/2000000(3%) Passed time:3h12m7s ETA:5d14h6m46s pred loss1:0.02 pred loss2:0.03 pred loss3:6.0736e-04 episode_length:184.62 episode_reward:-1923.09 critic_loss:22.66 max_critic_abs_err:26.05 policy_loss:194.92 alpha:0.02 alpha_loss:1.4367e-03 q:-195.59 q_target:-194.43 log_pi:12.91 memory:51.39G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:12:42 - 52240/2000000(3%) Passed time:3h18m35s ETA:5d13h54m39s pred loss1:0.07 pred loss2:0.02 pred loss3:5.9137e-04 episode_length:172.78 episode_reward:-1786.94 critic_loss:30.85 max_critic_abs_err:44.71 policy_loss:197.42 alpha:0.02 alpha_loss:-0.03 q:-197.67 q_target:-196.53 log_pi:14.60 memory:51.53G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:19:09 - 53840/2000000(3%) Passed time:3h25m ETA:5d13h42m33s pred loss1:0.05 pred loss2:0.02 pred loss3:5.9353e-04 episode_length:184.78 episode_reward:-2008.76 critic_loss:54.09 max_critic_abs_err:59.23 policy_loss:199.29 alpha:0.02 alpha_loss:0.03 q:-199.66 q_target:-198.32 log_pi:11.42 memory:51.65G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:25:34 - 55440/2000000(3%) Passed time:3h31m27s ETA:5d13h30m2s pred loss1:0.05 pred loss2:0.02 pred loss3:6.0319e-04 episode_length:200.00 episode_reward:-2197.35 critic_loss:24.57 max_critic_abs_err:32.57 policy_loss:199.42 alpha:0.02 alpha_loss:0.01 q:-200.17 q_target:-199.22 log_pi:12.27 memory:51.76G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:31:58 - 57040/2000000(3%) Passed time:3h37m50s ETA:5d13h16m30s pred loss1:0.03 pred loss2:0.04 pred loss3:5.8237e-04 episode_length:186.75 episode_reward:-1986.36 critic_loss:20.01 max_critic_abs_err:28.64 policy_loss:202.42 alpha:0.01 alpha_loss:0.02 q:-203.04 q_target:-202.03 log_pi:11.39 memory:51.89G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:38:25 - 58640/2000000(3%) Passed time:3h44m18s ETA:5d13h5m52s pred loss1:0.03 pred loss2:0.03 pred loss3:6.3516e-04 episode_length:176.22 episode_reward:-1855.71 critic_loss:19.48 max_critic_abs_err:26.50 policy_loss:201.03 alpha:0.01 alpha_loss:0.03 q:-201.55 q_target:-200.71 log_pi:11.11 memory:52.01G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:44:30 - 60144/2000000(3%) Passed time:3h50m23s ETA:5d12h56m40s pred loss1:0.03 pred loss2:0.03 pred loss3:5.8833e-04 episode_length:163.38 episode_reward:-1697.97 critic_loss:17.28 max_critic_abs_err:24.73 policy_loss:203.01 alpha:0.01 alpha_loss:0.02 q:-203.38 q_target:-202.59 log_pi:11.69 memory:52.11G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:44:30 - Save model at step: 60144.The model will be saved at ./work_dirs/OpenCabinetDrawer-v0/MBPO/models/model_60000.ckpt
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:49:01 - 61224/2000000(3%) Passed time:3h54m53s ETA:5d12h54m54s pred loss1:0.06 pred loss2:0.03 pred loss3:6.0832e-04 episode_length:153.50 episode_reward:-1616.83 critic_loss:25.37 max_critic_abs_err:35.24 policy_loss:202.98 alpha:0.01 alpha_loss:-0.01 q:-203.33 q_target:-202.41 log_pi:13.98 memory:52.20G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 13:55:29 - 62824/2000000(3%) Passed time:4h1m22s ETA:5d12h45m2s pred loss1:0.10 pred loss2:0.03 pred loss3:6.1511e-04 episode_length:171.25 episode_reward:-1829.82 critic_loss:45.47 max_critic_abs_err:41.51 policy_loss:205.30 alpha:0.01 alpha_loss:9.9974e-04 q:-205.61 q_target:-203.66 log_pi:12.93 memory:52.57G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 14:00:29 - 64048/2000000(3%) Passed time:4h6m22s ETA:5d12h39m27s pred loss1:0.06 pred loss2:0.03 pred loss3:6.0897e-04 episode_length:184.62 episode_reward:-1914.97 critic_loss:29.54 max_critic_abs_err:29.21 policy_loss:204.79 alpha:0.01 alpha_loss:7.8033e-03 q:-205.12 q_target:-204.33 log_pi:12.42 memory:52.68G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 14:06:58 - 65648/2000000(3%) Passed time:4h12m50s ETA:5d12h29m58s pred loss1:0.05 pred loss2:0.02 pred loss3:6.2908e-04 episode_length:200.00 episode_reward:-2081.65 critic_loss:36.57 max_critic_abs_err:38.54 policy_loss:205.39 alpha:0.01 alpha_loss:4.3103e-03 q:-205.51 q_target:-204.43 log_pi:12.66 memory:52.94G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 14:12:29 - 67000/2000000(3%) Passed time:4h18m22s ETA:5d12h23m58s pred loss1:0.06 pred loss2:0.03 pred loss3:6.1854e-04 episode_length:167.75 episode_reward:-1769.90 critic_loss:53.48 max_critic_abs_err:51.82 policy_loss:203.73 alpha:0.01 alpha_loss:-0.01 q:-204.11 q_target:-202.54 log_pi:13.81 memory:53.30G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 14:17:14 - 68144/2000000(3%) Passed time:4h23m6s ETA:5d12h20m36s pred loss1:0.07 pred loss2:0.02 pred loss3:6.1241e-04 episode_length:173.75 episode_reward:-1786.92 critic_loss:29.34 max_critic_abs_err:36.30 policy_loss:204.02 alpha:0.01 alpha_loss:5.5581e-03 q:-204.86 q_target:-203.47 log_pi:12.59 memory:53.40G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 14:23:42 - 69744/2000000(3%) Passed time:4h29m34s ETA:5d12h11m15s pred loss1:0.08 pred loss2:0.03 pred loss3:6.1699e-04 episode_length:192.75 episode_reward:-2125.01 critic_loss:20.69 max_critic_abs_err:23.24 policy_loss:207.94 alpha:0.01 alpha_loss:3.1338e-03 q:-208.24 q_target:-207.18 log_pi:12.79 memory:53.54G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 14:30:09 - 71344/2000000(4%) Passed time:4h36m2s ETA:5d12h1m51s pred loss1:0.02 pred loss2:0.02 pred loss3:6.0531e-04 episode_length:200.00 episode_reward:-2198.97 critic_loss:19.71 max_critic_abs_err:27.75 policy_loss:209.11 alpha:0.01 alpha_loss:5.3741e-03 q:-209.34 q_target:-208.63 log_pi:12.64 memory:53.67G gpu_mem_ratio:20.6% gpu_mem:6.53G gpu_mem_this:6.52G gpu_util:0%
