OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:27:48 - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0,1: Tesla V100S-PCIE-32GB
CUDA_HOME: /usr/local/cuda-11.2
NVCC: Build cuda_11.2.r11.2/compiler.29618528_0
Num of GPUs: 2
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.3-Product Build 20210617 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu102
OpenCV: 4.5.3
mani_skill_learn: 1.0.0
------------------------------------------------------------

OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:27:48 - Config:
agent = dict(
    type='SAC',
    batch_size=256,
    gamma=0.95,
    update_coeff=0.005,
    alpha=0.2,
    target_update_interval=1,
    automatic_alpha_tuning=True,
    alpha_optim_cfg=dict(type='Adam', lr=0.0003),
    policy_cfg=dict(
        type='ContinuousPolicy',
        policy_head_cfg=dict(
            type='GaussianHead', log_sig_min=-20, log_sig_max=2,
            epsilon=1e-06),
        nn_cfg=dict(
            type='PointNetWithInstanceInfoV0',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=['agent_shape + pcd_xyz_rgb_channel', 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[192, 128, 'action_shape * 2'],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0003, weight_decay=5e-06)),
    value_cfg=dict(
        type='ContinuousValue',
        num_heads=2,
        nn_cfg=dict(
            type='PointNetWithInstanceInfoV0',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=[
                        'agent_shape + pcd_xyz_rgb_channel + action_shape',
                        192, 192
                    ],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape + action_shape', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[192, 128, 1],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0005, weight_decay=5e-06)))
log_level = 'INFO'
train_mfrl_cfg = dict(
    on_policy=False,
    total_steps=2000000,
    warm_steps=4000,
    n_eval=50000,
    n_checkpoint=20000,
    n_steps=8,
    n_updates=4,
    m_steps=1)
rollout_cfg = dict(
    type='BatchRollout',
    use_cost=False,
    reward_only=False,
    num_procs=8,
    with_info=False,
    env_cfg=dict(
        type='gym',
        unwrapped=False,
        obs_mode='pointcloud',
        reward_type='dense',
        stack_frame=1,
        env_name='OpenCabinetDrawer-v0'))
eval_cfg = dict(
    type='BatchEvaluation',
    num=100,
    num_procs=8,
    use_hidden_state=False,
    start_state=None,
    save_traj=False,
    save_video=True,
    use_log=True,
    env_cfg=dict(
        type='gym',
        unwrapped=False,
        obs_mode='pointcloud',
        reward_type='dense',
        stack_frame=1,
        env_name='OpenCabinetDrawer-v0'))
stack_frame = 1
num_heads = 4
env_cfg = dict(
    type='gym',
    unwrapped=False,
    obs_mode='pointcloud',
    reward_type='dense',
    stack_frame=1,
    env_name='OpenCabinetDrawer-v0')
replay_cfg = dict(type='ReplayMemory', capacity=500000)
expert_replay_split_cfg = dict(type='ReplayMemory', capacity=30000)
resume_from = 'MBPO.ckpt'
work_dir = './work_dirs/OpenCabinetDrawer-v0/SAC'

OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:27:48 - Set random seed to 0
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:28:14 - State shape:{'pointcloud': {'rgb': (1200, 3), 'xyz': (1200, 3), 'seg': (1200, 3)}, 'state': 38}, action shape:Box(-1.0, 1.0, (13,), float32)
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:28:18 - We do not use distributed training, but we support data parallel in torch
OpenCabinetDrawer-v0 - WARNING - 2022-05-04 22:28:18 - Use Data parallel to train model! It may slow down the speed when the model is small.
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:28:18 - SAC(
  (policy): CustomDataParallel(
    (module): ContinuousPolicy(
      (backbone): PointNetWithInstanceInfoV0(
        (pcd_pns): ModuleList(
          (0): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (1): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (2): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (3): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (4): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
        )
        (attn): TransformerEncoder(
          (attn_blocks): ModuleList(
            (0): TransformerBlock(
              (attn): MultiHeadSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=768, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=768, out_features=192, bias=True)
                )
              )
              (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): TransformerBlock(
              (attn): MultiHeadSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=768, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=768, out_features=192, bias=True)
                )
              )
              (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (pooling): AttentionPooling(
            (dropout): Identity()
          )
        )
        (state_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=38, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=128, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=128, out_features=26, bias=True)
          )
        )
      )
      (policy_head): GaussianHead()
    )
  )
  (critic): CustomDataParallel(
    (module): ContinuousValue(
      (values): ModuleList(
        (0): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
        (1): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
      )
    )
  )
  (target_critic): CustomDataParallel(
    (module): ContinuousValue(
      (values): ModuleList(
        (0): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
        (1): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
      )
    )
  )
)
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:28:18 - Rollout state dim: {'pointcloud': {'rgb': (8, 1200, 3), 'xyz': (8, 1200, 3), 'seg': (8, 1200, 3)}, 'state': (8, 38)}, action dim: 8
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:29:45 - Finish 4000 warm-up steps!
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:36:00 - 5600/2000000(0%) Passed time:6m15s ETA:5d10h13m55s episode_length:171.50 episode_reward:-1643.28 critic_loss:2.67 max_critic_abs_err:10.94 policy_loss:177.29 alpha:0.02 alpha_loss:-1.6200e-03 q:-177.68 q_target:-177.39 log_pi:13.10 memory:40.64G gpu_mem_ratio:29.3% gpu_mem:9.32G gpu_mem_this:4.77G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:42:27 - 7200/2000000(0%) Passed time:12m42s ETA:5d12h6m23s episode_length:119.78 episode_reward:-1102.86 critic_loss:4.26 max_critic_abs_err:13.75 policy_loss:171.06 alpha:0.02 alpha_loss:-0.02 q:-171.76 q_target:-171.33 log_pi:14.15 memory:41.27G gpu_mem_ratio:30.1% gpu_mem:9.57G gpu_mem_this:4.77G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:49:01 - 8800/2000000(0%) Passed time:19m16s ETA:5d13h33m45s episode_length:188.11 episode_reward:-1753.94 critic_loss:3.90 max_critic_abs_err:8.53 policy_loss:175.13 alpha:0.02 alpha_loss:9.2670e-03 q:-175.52 q_target:-174.98 log_pi:12.50 memory:41.89G gpu_mem_ratio:32.5% gpu_mem:10.32G gpu_mem_this:5.27G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 22:55:41 - 10400/2000000(1%) Passed time:25m56s ETA:5d14h42m25s episode_length:171.22 episode_reward:-1752.04 critic_loss:4.66 max_critic_abs_err:9.35 policy_loss:174.85 alpha:0.02 alpha_loss:-8.5335e-03 q:-175.46 q_target:-175.00 log_pi:13.45 memory:42.64G gpu_mem_ratio:33.3% gpu_mem:10.57G gpu_mem_this:5.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:02:20 - 12000/2000000(1%) Passed time:32m35s ETA:5d15h13m46s episode_length:200.00 episode_reward:-2013.27 critic_loss:35.19 max_critic_abs_err:42.59 policy_loss:175.08 alpha:0.02 alpha_loss:0.02 q:-175.79 q_target:-174.63 log_pi:12.15 memory:43.34G gpu_mem_ratio:33.3% gpu_mem:10.57G gpu_mem_this:5.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:08:42 - 13544/2000000(1%) Passed time:38m56s ETA:5d15h23m13s episode_length:173.25 episode_reward:-1762.81 critic_loss:24.27 max_critic_abs_err:34.47 policy_loss:175.57 alpha:0.02 alpha_loss:0.01 q:-176.50 q_target:-175.57 log_pi:12.44 memory:44.01G gpu_mem_ratio:34.9% gpu_mem:11.07G gpu_mem_this:6.02G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:15:26 - 15144/2000000(1%) Passed time:45m41s ETA:5d15h54m59s episode_length:191.88 episode_reward:-2001.01 critic_loss:13.41 max_critic_abs_err:21.18 policy_loss:180.51 alpha:0.02 alpha_loss:0.02 q:-181.22 q_target:-180.29 log_pi:12.07 memory:44.44G gpu_mem_ratio:35.6% gpu_mem:11.32G gpu_mem_this:6.02G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:21:55 - 16688/2000000(1%) Passed time:52m10s ETA:5d16h12m48s episode_length:196.88 episode_reward:-2023.77 critic_loss:17.52 max_critic_abs_err:21.34 policy_loss:183.63 alpha:0.02 alpha_loss:0.02 q:-184.19 q_target:-183.34 log_pi:11.82 memory:45.25G gpu_mem_ratio:37.2% gpu_mem:11.82G gpu_mem_this:6.27G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:28:37 - 18288/2000000(1%) Passed time:58m52s ETA:5d16h21m49s episode_length:200.00 episode_reward:-2143.71 critic_loss:15.87 max_critic_abs_err:26.02 policy_loss:186.22 alpha:0.02 alpha_loss:0.02 q:-186.83 q_target:-186.02 log_pi:12.11 memory:45.38G gpu_mem_ratio:37.2% gpu_mem:11.82G gpu_mem_this:6.27G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:34:46 - 19744/2000000(1%) Passed time:1h5m ETA:5d16h35m17s episode_length:191.62 episode_reward:-1982.26 critic_loss:18.86 max_critic_abs_err:20.27 policy_loss:188.84 alpha:0.02 alpha_loss:0.01 q:-189.52 q_target:-188.48 log_pi:12.42 memory:45.97G gpu_mem_ratio:38.0% gpu_mem:12.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:40:40 - 21144/2000000(1%) Passed time:1h10m55s ETA:5d16h43m5s episode_length:187.75 episode_reward:-1941.21 critic_loss:17.73 max_critic_abs_err:29.07 policy_loss:189.01 alpha:0.02 alpha_loss:-0.02 q:-189.59 q_target:-188.51 log_pi:13.93 memory:46.35G gpu_mem_ratio:38.8% gpu_mem:12.32G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:40:40 - Save model at step: 21144.The model will be saved at ./work_dirs/OpenCabinetDrawer-v0/SAC/models/model_20000.ckpt
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:45:27 - 22288/2000000(1%) Passed time:1h15m42s ETA:5d16h44m17s episode_length:154.75 episode_reward:-1555.23 critic_loss:27.50 max_critic_abs_err:37.53 policy_loss:188.35 alpha:0.02 alpha_loss:3.5567e-03 q:-189.15 q_target:-188.09 log_pi:12.80 memory:46.53G gpu_mem_ratio:38.8% gpu_mem:12.32G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:52:12 - 23888/2000000(1%) Passed time:1h22m27s ETA:5d16h49m17s episode_length:184.25 episode_reward:-1787.82 critic_loss:15.30 max_critic_abs_err:19.74 policy_loss:190.22 alpha:0.02 alpha_loss:-2.6279e-04 q:-190.58 q_target:-189.67 log_pi:13.01 memory:46.92G gpu_mem_ratio:38.8% gpu_mem:12.32G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-04 23:58:58 - 25488/2000000(1%) Passed time:1h29m13s ETA:5d16h54m57s episode_length:188.62 episode_reward:-1960.98 critic_loss:25.68 max_critic_abs_err:34.59 policy_loss:193.99 alpha:0.02 alpha_loss:0.05 q:-194.09 q_target:-192.79 log_pi:10.70 memory:47.23G gpu_mem_ratio:38.8% gpu_mem:12.32G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:03:38 - 26600/2000000(1%) Passed time:1h33m53s ETA:5d16h54m35s episode_length:179.38 episode_reward:-1990.10 critic_loss:40.03 max_critic_abs_err:38.93 policy_loss:199.03 alpha:0.02 alpha_loss:0.03 q:-199.14 q_target:-197.32 log_pi:11.64 memory:47.36G gpu_mem_ratio:39.6% gpu_mem:12.57G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:10:18 - 28200/2000000(1%) Passed time:1h40m32s ETA:5d16h49m21s episode_length:189.88 episode_reward:-2219.27 critic_loss:31.21 max_critic_abs_err:38.76 policy_loss:200.01 alpha:0.02 alpha_loss:-0.04 q:-200.47 q_target:-199.09 log_pi:15.27 memory:47.57G gpu_mem_ratio:39.6% gpu_mem:12.57G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:17:03 - 29800/2000000(1%) Passed time:1h47m18s ETA:5d16h50m52s episode_length:181.62 episode_reward:-1987.30 critic_loss:38.20 max_critic_abs_err:47.95 policy_loss:203.86 alpha:0.02 alpha_loss:-0.03 q:-204.22 q_target:-202.91 log_pi:14.62 memory:47.73G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:21:08 - 30752/2000000(2%) Passed time:1h51m23s ETA:5d16h56m2s episode_length:160.75 episode_reward:-1720.01 critic_loss:19.85 max_critic_abs_err:31.97 policy_loss:204.05 alpha:0.02 alpha_loss:0.02 q:-205.12 q_target:-203.82 log_pi:12.07 memory:48.11G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:27:30 - 32248/2000000(2%) Passed time:1h57m45s ETA:5d16h59m29s episode_length:148.88 episode_reward:-1553.88 critic_loss:23.10 max_critic_abs_err:33.61 policy_loss:204.29 alpha:0.02 alpha_loss:0.01 q:-205.18 q_target:-203.92 log_pi:12.33 memory:48.25G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:33:15 - 33584/2000000(2%) Passed time:2h3m29s ETA:5d17h5m34s episode_length:185.38 episode_reward:-1918.16 critic_loss:16.29 max_critic_abs_err:19.78 policy_loss:204.35 alpha:0.02 alpha_loss:0.03 q:-205.14 q_target:-203.82 log_pi:11.49 memory:48.47G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:40:03 - 35184/2000000(2%) Passed time:2h10m18s ETA:5d17h7m15s episode_length:199.38 episode_reward:-2076.29 critic_loss:19.02 max_critic_abs_err:18.48 policy_loss:204.88 alpha:0.02 alpha_loss:0.03 q:-205.34 q_target:-204.38 log_pi:11.13 memory:48.66G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:46:17 - 36640/2000000(2%) Passed time:2h16m32s ETA:5d17h9m38s episode_length:167.00 episode_reward:-1715.43 critic_loss:19.40 max_critic_abs_err:22.65 policy_loss:203.52 alpha:0.02 alpha_loss:-0.03 q:-204.27 q_target:-203.11 log_pi:14.93 memory:48.85G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:53:11 - 38240/2000000(2%) Passed time:2h23m26s ETA:5d17h14m59s episode_length:183.12 episode_reward:-1880.37 critic_loss:18.45 max_critic_abs_err:21.67 policy_loss:200.82 alpha:0.02 alpha_loss:-2.9651e-03 q:-201.75 q_target:-200.62 log_pi:13.17 memory:48.98G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 00:58:28 - 39456/2000000(2%) Passed time:2h28m43s ETA:5d17h20m56s episode_length:160.62 episode_reward:-1667.72 critic_loss:22.94 max_critic_abs_err:27.79 policy_loss:199.60 alpha:0.02 alpha_loss:4.7145e-03 q:-200.75 q_target:-199.71 log_pi:12.73 memory:49.09G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:05:24 - 41056/2000000(2%) Passed time:2h35m39s ETA:5d17h25m36s episode_length:200.00 episode_reward:-1948.42 critic_loss:27.46 max_critic_abs_err:36.68 policy_loss:198.73 alpha:0.02 alpha_loss:-0.04 q:-199.81 q_target:-198.76 log_pi:15.35 memory:49.21G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:05:24 - Save model at step: 41056.The model will be saved at ./work_dirs/OpenCabinetDrawer-v0/SAC/models/model_40000.ckpt
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:12:16 - 42640/2000000(2%) Passed time:2h42m31s ETA:5d17h29m53s episode_length:180.50 episode_reward:-1787.67 critic_loss:22.66 max_critic_abs_err:33.86 policy_loss:197.44 alpha:0.02 alpha_loss:6.0873e-03 q:-198.72 q_target:-197.78 log_pi:12.66 memory:49.57G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:19:07 - 44216/2000000(2%) Passed time:2h49m22s ETA:5d17h33m31s episode_length:183.00 episode_reward:-1805.09 critic_loss:23.44 max_critic_abs_err:21.85 policy_loss:194.21 alpha:0.02 alpha_loss:7.3751e-03 q:-195.90 q_target:-194.61 log_pi:12.59 memory:49.95G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:25:55 - 45784/2000000(2%) Passed time:2h56m10s ETA:5d17h36m43s episode_length:192.88 episode_reward:-1941.65 critic_loss:20.83 max_critic_abs_err:23.51 policy_loss:195.00 alpha:0.02 alpha_loss:7.2160e-03 q:-196.16 q_target:-195.18 log_pi:12.61 memory:50.08G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:31:26 - 47048/2000000(2%) Passed time:3h1m41s ETA:5d17h39m53s episode_length:184.75 episode_reward:-1794.67 critic_loss:21.63 max_critic_abs_err:24.13 policy_loss:193.49 alpha:0.02 alpha_loss:-5.9661e-03 q:-194.31 q_target:-193.30 log_pi:13.34 memory:50.69G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:38:22 - 48648/2000000(2%) Passed time:3h8m37s ETA:5d17h40m57s episode_length:200.00 episode_reward:-1978.99 critic_loss:27.18 max_critic_abs_err:38.16 policy_loss:195.32 alpha:0.02 alpha_loss:0.01 q:-196.19 q_target:-194.77 log_pi:12.18 memory:50.87G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:45:16 - 50248/2000000(3%) Passed time:3h15m31s ETA:5d17h39m42s episode_length:200.00 episode_reward:-2092.43 critic_loss:33.97 max_critic_abs_err:37.20 policy_loss:199.01 alpha:0.02 alpha_loss:0.01 q:-199.51 q_target:-198.43 log_pi:12.33 memory:51.14G gpu_mem_ratio:41.2% gpu_mem:13.07G gpu_mem_this:6.52G gpu_util:0%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:45:16 - Begin to evaluate at step: 50248. The evaluation info will be saved at eval_50000
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:45:49 - Resource usage: memory:55.07G gpu_mem_ratio:41.1% gpu_mem:13.06G gpu_mem_this:6.52G gpu_util:3%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:46:18 - Resource usage: memory:56.11G gpu_mem_ratio:41.1% gpu_mem:13.06G gpu_mem_this:6.52G gpu_util:10%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:46:49 - Resource usage: memory:57.23G gpu_mem_ratio:41.2% gpu_mem:13.06G gpu_mem_this:6.52G gpu_util:4%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:47:20 - Resource usage: memory:57.96G gpu_mem_ratio:41.1% gpu_mem:13.06G gpu_mem_this:6.52G gpu_util:15%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:47:52 - Resource usage: memory:58.96G gpu_mem_ratio:41.1% gpu_mem:13.06G gpu_mem_this:6.52G gpu_util:7%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:48:23 - Resource usage: memory:59.88G gpu_mem_ratio:42.7% gpu_mem:13.56G gpu_mem_this:7.02G gpu_util:1%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:48:56 - Resource usage: memory:60.68G gpu_mem_ratio:43.5% gpu_mem:13.81G gpu_mem_this:7.27G gpu_util:11%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:49:30 - Resource usage: memory:61.09G gpu_mem_ratio:42.7% gpu_mem:13.56G gpu_mem_this:7.02G gpu_util:12%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:50:01 - Resource usage: memory:61.68G gpu_mem_ratio:42.7% gpu_mem:13.56G gpu_mem_this:7.02G gpu_util:22%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:50:35 - Resource usage: memory:62.07G gpu_mem_ratio:45.1% gpu_mem:14.31G gpu_mem_this:7.77G gpu_util:8%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:51:11 - Resource usage: memory:62.38G gpu_mem_ratio:45.9% gpu_mem:14.57G gpu_mem_this:8.02G gpu_util:8%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:51:43 - Resource usage: memory:62.73G gpu_mem_ratio:46.7% gpu_mem:14.81G gpu_mem_this:8.27G gpu_util:6%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:52:09 - Resource usage: memory:62.76G gpu_mem_ratio:46.7% gpu_mem:14.81G gpu_mem_this:8.27G gpu_util:1%
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:52:10 - Merge videos to ./work_dirs/OpenCabinetDrawer-v0/SAC/eval_50000/videos
OpenCabinetDrawer-v0 - INFO - 2022-05-05 01:52:10 - Num of trails: 100.00, Length: 197.83+/-14.68, Reward: -2064.64+/-166.57, Success or Early Stop Rate: 0.03
