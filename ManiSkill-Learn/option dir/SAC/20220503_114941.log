OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:49:43 - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100S-PCIE-32GB
CUDA_HOME: /usr/local/cuda-11.2
NVCC: Build cuda_11.2.r11.2/compiler.29618528_0
Num of GPUs: 8
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.3-Product Build 20210617 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu102
OpenCV: 4.5.3
mani_skill_learn: 1.0.0
------------------------------------------------------------

OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:49:44 - Config:
agent = dict(
    type='SAC',
    batch_size=256,
    gamma=0.95,
    update_coeff=0.005,
    alpha=0.2,
    target_update_interval=1,
    automatic_alpha_tuning=True,
    alpha_optim_cfg=dict(type='Adam', lr=0.0003),
    policy_cfg=dict(
        type='ContinuousPolicy',
        policy_head_cfg=dict(
            type='GaussianHead', log_sig_min=-20, log_sig_max=2,
            epsilon=1e-06),
        nn_cfg=dict(
            type='PointNetWithInstanceInfoV0',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=['agent_shape + pcd_xyz_rgb_channel', 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[192, 128, 'action_shape * 2'],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0003, weight_decay=5e-06)),
    value_cfg=dict(
        type='ContinuousValue',
        num_heads=2,
        nn_cfg=dict(
            type='PointNetWithInstanceInfoV0',
            stack_frame=1,
            num_objs='num_objs',
            pcd_pn_cfg=dict(
                type='PointNetV0',
                conv_cfg=dict(
                    type='ConvMLP',
                    norm_cfg=None,
                    mlp_spec=[
                        'agent_shape + pcd_xyz_rgb_channel + action_shape',
                        192, 192
                    ],
                    bias='auto',
                    inactivated_output=True,
                    conv_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                mlp_cfg=dict(
                    type='LinearMLP',
                    norm_cfg=None,
                    mlp_spec=[192, 192, 192],
                    bias='auto',
                    inactivated_output=True,
                    linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
                subtract_mean_coords=True,
                max_mean_mix_aggregation=True),
            state_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=['agent_shape + action_shape', 192, 192],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0)),
            transformer_cfg=dict(
                type='TransformerEncoder',
                block_cfg=dict(
                    attention_cfg=dict(
                        type='MultiHeadSelfAttention',
                        embed_dim=192,
                        num_heads=4,
                        latent_dim=32,
                        dropout=0.1),
                    mlp_cfg=dict(
                        type='LinearMLP',
                        norm_cfg=None,
                        mlp_spec=[192, 768, 192],
                        bias='auto',
                        inactivated_output=True,
                        linear_init_cfg=dict(
                            type='xavier_init', gain=1, bias=0)),
                    dropout=0.1),
                pooling_cfg=dict(embed_dim=192, num_heads=4, latent_dim=32),
                mlp_cfg=None,
                num_blocks=2),
            final_mlp_cfg=dict(
                type='LinearMLP',
                norm_cfg=None,
                mlp_spec=[192, 128, 1],
                bias='auto',
                inactivated_output=True,
                linear_init_cfg=dict(type='xavier_init', gain=1, bias=0))),
        optim_cfg=dict(type='Adam', lr=0.0005, weight_decay=5e-06)))
log_level = 'INFO'
train_mfrl_cfg = dict(
    on_policy=False,
    total_steps=2000000,
    warm_steps=4000,
    n_eval=2000000,
    n_checkpoint=100000,
    n_steps=8,
    n_updates=4,
    m_steps=1)
rollout_cfg = dict(
    type='BatchRollout',
    use_cost=False,
    reward_only=False,
    num_procs=8,
    with_info=False,
    env_cfg=dict(
        type='gym',
        unwrapped=False,
        obs_mode='pointcloud',
        reward_type='dense',
        stack_frame=1,
        env_name='OpenCabinetDrawer-v0'))
eval_cfg = dict(
    type='BatchEvaluation',
    num=100,
    num_procs=8,
    use_hidden_state=False,
    start_state=None,
    save_traj=False,
    save_video=True,
    use_log=True,
    env_cfg=dict(
        type='gym',
        unwrapped=False,
        obs_mode='pointcloud',
        reward_type='dense',
        stack_frame=1,
        env_name='OpenCabinetDrawer-v0'))
stack_frame = 1
num_heads = 4
env_cfg = dict(
    type='gym',
    unwrapped=False,
    obs_mode='pointcloud',
    reward_type='dense',
    stack_frame=1,
    env_name='OpenCabinetDrawer-v0')
replay_cfg = dict(type='ReplayMemory', capacity=500000)
work_dir = 'option dir/SAC'
resume_from = 'SAC.ckpt'

OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:49:44 - Set random seed to 0
OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:50:02 - State shape:{'pointcloud': {'rgb': (1200, 3), 'xyz': (1200, 3), 'seg': (1200, 3)}, 'state': 38}, action shape:Box(-1.0, 1.0, (13,), float32)
OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:50:06 - We do not use distributed training, but we support data parallel in torch
OpenCabinetDrawer-v0 - WARNING - 2022-05-03 11:50:06 - Use Data parallel to train model! It may slow down the speed when the model is small.
OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:50:06 - SAC(
  (policy): CustomDataParallel(
    (module): ContinuousPolicy(
      (backbone): PointNetWithInstanceInfoV0(
        (pcd_pns): ModuleList(
          (0): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (1): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (2): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (3): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
          (4): PointNetV0(
            (conv_mlp): ConvMLP(
              (mlp): Sequential(
                (layer0): ConvModule(
                  (conv): Conv1d(47, 192, kernel_size=(1,), stride=(1,))
                  (activate): ReLU(inplace=True)
                )
                (layer1): ConvModule(
                  (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                )
              )
            )
            (global_mlp): LinearMLP(
              (mlp): Sequential(
                (linear0): Linear(in_features=192, out_features=192, bias=True)
                (act0): ReLU()
                (linear1): Linear(in_features=192, out_features=192, bias=True)
              )
            )
          )
        )
        (attn): TransformerEncoder(
          (attn_blocks): ModuleList(
            (0): TransformerBlock(
              (attn): MultiHeadSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=768, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=768, out_features=192, bias=True)
                )
              )
              (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): TransformerBlock(
              (attn): MultiHeadSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=768, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=768, out_features=192, bias=True)
                )
              )
              (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (pooling): AttentionPooling(
            (dropout): Identity()
          )
        )
        (state_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=38, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=128, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=128, out_features=26, bias=True)
          )
        )
      )
      (policy_head): GaussianHead()
    )
  )
  (critic): CustomDataParallel(
    (module): ContinuousValue(
      (values): ModuleList(
        (0): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
        (1): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
      )
    )
  )
  (target_critic): CustomDataParallel(
    (module): ContinuousValue(
      (values): ModuleList(
        (0): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
        (1): PointNetWithInstanceInfoV0(
          (pcd_pns): ModuleList(
            (0): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (1): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (2): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (3): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
            (4): PointNetV0(
              (conv_mlp): ConvMLP(
                (mlp): Sequential(
                  (layer0): ConvModule(
                    (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
                    (activate): ReLU(inplace=True)
                  )
                  (layer1): ConvModule(
                    (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
                  )
                )
              )
              (global_mlp): LinearMLP(
                (mlp): Sequential(
                  (linear0): Linear(in_features=192, out_features=192, bias=True)
                  (act0): ReLU()
                  (linear1): Linear(in_features=192, out_features=192, bias=True)
                )
              )
            )
          )
          (attn): TransformerEncoder(
            (attn_blocks): ModuleList(
              (0): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): TransformerBlock(
                (attn): MultiHeadSelfAttention(
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (mlp): LinearMLP(
                  (mlp): Sequential(
                    (linear0): Linear(in_features=192, out_features=768, bias=True)
                    (act0): ReLU()
                    (linear1): Linear(in_features=768, out_features=192, bias=True)
                  )
                )
                (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (pooling): AttentionPooling(
              (dropout): Identity()
            )
          )
          (state_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=51, out_features=192, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=192, out_features=192, bias=True)
            )
          )
          (global_mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=128, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=128, out_features=1, bias=True)
            )
          )
        )
      )
    )
  )
)
OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:50:06 - Rollout state dim: {'pointcloud': {'rgb': (8, 1200, 3), 'xyz': (8, 1200, 3), 'seg': (8, 1200, 3)}, 'state': (8, 38)}, action dim: 8
OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:51:21 - Finish 4000 warm-up steps!
OpenCabinetDrawer-v0 - INFO - 2022-05-03 11:57:24 - 5600/2000000(0%) Passed time:6m2s ETA:5d5h47m11s episode_length:200.00 episode_reward:-1853.01 critic_loss:0.15 max_critic_abs_err:1.16 policy_loss:172.58 alpha:8.2987e-03 alpha_loss:-0.03 q:-172.91 q_target:-172.82 log_pi:16.38 memory:37.09G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:70%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:03:24 - 7200/2000000(0%) Passed time:12m2s ETA:5d5h18m33s episode_length:200.00 episode_reward:-1761.34 critic_loss:0.14 max_critic_abs_err:1.48 policy_loss:171.94 alpha:0.01 alpha_loss:-0.03 q:-172.42 q_target:-172.31 log_pi:15.60 memory:38.01G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:82%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:09:25 - 8800/2000000(0%) Passed time:18m4s ETA:5d5h11m episode_length:200.00 episode_reward:-1740.57 critic_loss:0.14 max_critic_abs_err:1.34 policy_loss:171.41 alpha:0.01 alpha_loss:-0.02 q:-171.79 q_target:-171.68 log_pi:14.33 memory:39.10G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:83%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:15:31 - 10400/2000000(1%) Passed time:24m10s ETA:5d5h28m56s episode_length:200.00 episode_reward:-1788.40 critic_loss:0.20 max_critic_abs_err:1.76 policy_loss:171.62 alpha:0.01 alpha_loss:-5.2308e-03 q:-172.03 q_target:-171.89 log_pi:13.36 memory:39.85G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:83%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:21:30 - 12000/2000000(1%) Passed time:30m9s ETA:5d5h8m20s episode_length:200.00 episode_reward:-1762.32 critic_loss:0.27 max_critic_abs_err:2.74 policy_loss:171.28 alpha:0.01 alpha_loss:-5.0561e-03 q:-171.65 q_target:-171.50 log_pi:13.36 memory:40.79G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:75%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:27:33 - 13600/2000000(1%) Passed time:36m11s ETA:5d5h4m24s episode_length:200.00 episode_reward:-1730.28 critic_loss:0.20 max_critic_abs_err:2.20 policy_loss:171.29 alpha:0.01 alpha_loss:-3.5520e-03 q:-171.58 q_target:-171.47 log_pi:13.24 memory:41.40G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:59%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:33:36 - 15200/2000000(1%) Passed time:42m15s ETA:5d5h3m8s episode_length:200.00 episode_reward:-1760.16 critic_loss:0.34 max_critic_abs_err:2.32 policy_loss:171.13 alpha:0.02 alpha_loss:-6.9024e-03 q:-171.40 q_target:-171.28 log_pi:13.44 memory:41.98G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:81%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:39:37 - 16800/2000000(1%) Passed time:48m16s ETA:5d4h53m55s episode_length:200.00 episode_reward:-1756.81 critic_loss:0.44 max_critic_abs_err:2.75 policy_loss:171.20 alpha:0.02 alpha_loss:-7.1836e-03 q:-171.53 q_target:-171.28 log_pi:13.44 memory:42.45G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:71%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:45:39 - 18400/2000000(1%) Passed time:54m18s ETA:5d4h47m24s episode_length:200.00 episode_reward:-1748.49 critic_loss:0.68 max_critic_abs_err:5.34 policy_loss:170.86 alpha:0.02 alpha_loss:-5.4529e-03 q:-171.23 q_target:-170.96 log_pi:13.33 memory:42.63G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:84%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:51:40 - 20000/2000000(1%) Passed time:1h19s ETA:5d4h40m7s episode_length:188.12 episode_reward:-1727.59 critic_loss:2.05 max_critic_abs_err:6.34 policy_loss:170.91 alpha:0.02 alpha_loss:-0.01 q:-171.42 q_target:-171.09 log_pi:13.62 memory:43.48G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:81%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 12:57:42 - 21600/2000000(1%) Passed time:1h6m21s ETA:5d4h34m11s episode_length:200.00 episode_reward:-1934.54 critic_loss:3.04 max_critic_abs_err:8.06 policy_loss:171.17 alpha:0.02 alpha_loss:-0.03 q:-171.70 q_target:-171.29 log_pi:14.32 memory:43.73G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:82%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:03:46 - 23200/2000000(1%) Passed time:1h12m25s ETA:5d4h31m17s episode_length:191.50 episode_reward:-1951.64 critic_loss:39.82 max_critic_abs_err:42.53 policy_loss:173.93 alpha:0.02 alpha_loss:0.02 q:-174.37 q_target:-173.99 log_pi:12.21 memory:44.13G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:85%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:09:51 - 24800/2000000(1%) Passed time:1h18m30s ETA:5d4h30m7s episode_length:200.00 episode_reward:-2082.36 critic_loss:2.54 max_critic_abs_err:5.83 policy_loss:176.68 alpha:0.02 alpha_loss:-7.9489e-03 q:-177.22 q_target:-176.81 log_pi:13.40 memory:44.30G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:50%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:15:57 - 26400/2000000(1%) Passed time:1h24m36s ETA:5d4h29m3s episode_length:200.00 episode_reward:-2077.38 critic_loss:3.10 max_critic_abs_err:6.51 policy_loss:178.79 alpha:0.02 alpha_loss:4.0005e-04 q:-179.46 q_target:-179.06 log_pi:12.98 memory:45.10G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:83%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:21:59 - 28000/2000000(1%) Passed time:1h30m37s ETA:5d4h21m57s episode_length:200.00 episode_reward:-2067.67 critic_loss:3.37 max_critic_abs_err:7.88 policy_loss:180.11 alpha:0.03 alpha_loss:1.0936e-03 q:-180.87 q_target:-180.46 log_pi:12.96 memory:45.42G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:84%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:28:01 - 29600/2000000(1%) Passed time:1h36m40s ETA:5d4h15m49s episode_length:168.75 episode_reward:-1604.42 critic_loss:4.77 max_critic_abs_err:10.36 policy_loss:179.42 alpha:0.03 alpha_loss:0.01 q:-180.22 q_target:-179.74 log_pi:12.63 memory:45.86G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:89%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:34:05 - 31200/2000000(2%) Passed time:1h42m44s ETA:5d4h11m42s episode_length:200.00 episode_reward:-1940.01 critic_loss:8.29 max_critic_abs_err:16.27 policy_loss:177.41 alpha:0.03 alpha_loss:-0.01 q:-178.37 q_target:-177.90 log_pi:13.37 memory:46.69G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:83%
OpenCabinetDrawer-v0 - INFO - 2022-05-03 13:40:07 - 32800/2000000(2%) Passed time:1h48m45s ETA:5d4h4m20s episode_length:198.00 episode_reward:-1904.43 critic_loss:5.94 max_critic_abs_err:10.70 policy_loss:175.36 alpha:0.03 alpha_loss:-0.02 q:-176.65 q_target:-176.09 log_pi:13.57 memory:46.84G gpu_mem_ratio:13.6% gpu_mem:4.31G gpu_mem_this:4.31G gpu_util:87%
