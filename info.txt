ModuleList(
  (0): PointNetWithInstanceInfoV0(

  (1): PointNetWithInstanceInfoV0(
    (pcd_pns): ModuleList(
      (0): PointNetV0(
        (conv_mlp): ConvMLP(
          (mlp): Sequential(
            (layer0): ConvModule(
              (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
              (activate): ReLU(inplace=True)
            )
            (layer1): ConvModule(
              (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
            )
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
      )
      (1): PointNetV0(
        (conv_mlp): ConvMLP(
          (mlp): Sequential(
            (layer0): ConvModule(
              (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
              (activate): ReLU(inplace=True)
            )
            (layer1): ConvModule(
              (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
            )
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
      )
      (2): PointNetV0(
        (conv_mlp): ConvMLP(
          (mlp): Sequential(
            (layer0): ConvModule(
              (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
              (activate): ReLU(inplace=True)
            )
            (layer1): ConvModule(
              (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
            )
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
      )
      (3): PointNetV0(
        (conv_mlp): ConvMLP(
          (mlp): Sequential(
            (layer0): ConvModule(
              (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
              (activate): ReLU(inplace=True)
            )
            (layer1): ConvModule(
              (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
            )
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
      )
      (4): PointNetV0(
        (conv_mlp): ConvMLP(
          (mlp): Sequential(
            (layer0): ConvModule(
              (conv): Conv1d(60, 192, kernel_size=(1,), stride=(1,))
              (activate): ReLU(inplace=True)
            )
            (layer1): ConvModule(
              (conv): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
            )
          )
        )
        (global_mlp): LinearMLP(
          (mlp): Sequential(
            (linear0): Linear(in_features=192, out_features=192, bias=True)
            (act0): ReLU()
            (linear1): Linear(in_features=192, out_features=192, bias=True)
          )
        )
      )
    )
    (attn): TransformerEncoder(
      (attn_blocks): ModuleList(
        (0): TransformerBlock(
          (attn): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=768, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=768, out_features=192, bias=True)
            )
          )
          (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerBlock(
          (attn): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (mlp): LinearMLP(
            (mlp): Sequential(
              (linear0): Linear(in_features=192, out_features=768, bias=True)
              (act0): ReLU()
              (linear1): Linear(in_features=768, out_features=192, bias=True)
            )
          )
          (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (pooling): AttentionPooling(
        (dropout): Identity()
      )
    )
    (state_mlp): LinearMLP(
      (mlp): Sequential(
        (linear0): Linear(in_features=51, out_features=192, bias=True)
        (act0): ReLU()
        (linear1): Linear(in_features=192, out_features=192, bias=True)
      )
    )
    (global_mlp): LinearMLP(
      (mlp): Sequential(
        (linear0): Linear(in_features=192, out_features=128, bias=True)
        (act0): ReLU()
        (linear1): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
)